% You should title the file with a .tex extension (hw1.tex, for example)
\documentclass[11pt]{article}

\usepackage[latin1]{inputenc}
\usepackage{enumerate}
\usepackage[hang,flushmargin]{footmisc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{collectbox}
\usepackage{mathtools}
\usepackage{bm}
\usepackage[T1]{fontenc}
\usepackage{complexity}

\newcommand{\mybox}{%
    \collectbox{%
        \setlength{\fboxsep}{4pt}%
        \fbox{\BOXCONTENT}%
    }%
}

\newcommand{\legendre}[2]{\ensuremath{\left( \frac{#1}{#2} \right) }}

\oddsidemargin0cm
\topmargin-2cm     %I recommend adding these three lines to increase the 
\textwidth16.5cm   %amount of usable space on the page (and save trees)
\textheight23.5cm  

\newcommand{\question}[1] {\vspace{.3in} \hrule\vspace{0.3em}
\noindent{\bf #1} \vspace{0.7em}
\hrule \vspace{.10in}}
\renewcommand{\part}[1] {\vspace{.10in} {\bf (#1)}}

\newcommand{\myinfo}{Albert Gao / sixiangg}
\newcommand{\myhwnum}{1}
\newcommand{\currdate}{09/22/2022}

\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
 
\pagestyle{fancyplain}
\lhead{\fancyplain{}{\textbf{HW\myhwnum}}}      % Note the different brackets!
\rhead{\fancyplain{}{\bigskip\myinfo}}
\chead{\fancyplain{}{15-859}}

\begin{document}

\bigskip                        % Skip a "medium" amount of space
                                % (latex determines what medium is)
                                % Also try: \bigskip, \littleskip

\thispagestyle{plain}
\begin{center}                  % Center the following lines
{\Huge 15-859 Assignment \#\myhwnum}

\vspace{0.3cm}

\large{\myinfo { / section 1A}}

\large{\currdate}

\end{center}

\question{Task 1}
\begin{enumerate}[1.]
\item \begin{proof}
Let $z_i = x_iy_i$ then observe that
\begin{align*}
\langle x^{\otimes p}, y^{\otimes p}\rangle &= \sum_{i_1, i_2, \cdots i_p} \prod_{l \in [p]}x_{i_l}y_{i_l}\\
&= \sum_{i_1, i_2, \cdots, i_p} \prod_{l \in [p]} z_{i_l}\\
&= (z_1 + \cdots + z_d)^p\\
&= \langle x, y\rangle^p. \qedhere
\end{align*}
\end{proof}
\item \begin{proof}
Let $A_i$ denote $i$th row of $A$. Then for each $i \in [n]$, we have $M_i = A_i^{\otimes p/2}$. Then indeed $M$ is a $n \times d^{p/2}$ matrix. It is also clear that
\begin{align*}
    ||Mx^{\otimes p/2}||_2^2 &= \sum_{i \in [n]}{\langle M_i, x^{\otimes p/2}\rangle^2}\\
    &= \sum_{i \in [n]} \langle A_i, x \rangle^{p/2 \times 2}\\
    &= ||Ax||_p^p.
\end{align*}
To construct $M$, each entry is a $p/2$ product, so it takes $O(nd^{p/2}(p/2)) = n \cdot \text{poly}(d)$ time. Also, by discussion in class / problem statement, we may let $S$ be a random $s \times n$ Gaussian matrix, where $s = O(d^{p/2}/\epsilon^2)$, such that with probability at least $9/10$,
\begin{align*}
    \forall x \in \mathbb{R}^{d^(p/2)}, ||SMx||_2^2 &= (1 \pm \epsilon)||Mx||_2^2\\
    \Rightarrow \forall x \in \mathbb{R}^d, ||SMx^{\otimes p/2}||_2^2 &= (1 \pm \epsilon)||Mx^{\otimes p/2}||_2^2\\
    &= (1 \pm \epsilon)||Ax||_p^p.
\end{align*}
Finally, to compute $SM$ we need only to perform $O(d^{p/2}/\epsilon^2)d^{p/2}$ dot products of vectors of length $n$, which takes in total $n \cdot \text{poly}(d)$ time.
\end{proof}
\item \begin{proof}
In the special case when $A$ is Vandermonde, we no longer need all $d^{p/2}$ columns of $M$ because of repetitive entries. Let $S_k$ denote indices in $A_i^{\otimes p}$ for which the entries equal $y_i^k$. Then let $M'$ have rows $(1, y_i, \cdots, y_i^{p(d-1)})$. Let $y$ be a $p(d-1)+1$ length vector such that $y_k = \sum_{l \in S_k} x_l^{\otimes p/2}$. Then using same theorem in class / what the problem statement mentions, we can have matrix $S$ appropriatedly scaled Gaussian with $O(p(d-1)/\epsilon^2) = O(dp/\epsilon^2)$ rows such that with at least $9/10$ probability,
\begin{align*}
    ||SMx^{\otimes p/2}||_2^2 = ||SM'y||_2^2 &= (1 \pm \epsilon)||M'y||_2^2\\
    &= (1 \pm \epsilon)||Mx^{\otimes p/2}||_2^2\\
    &= (1 \pm \epsilon)||Ax||_p^p. \qedhere
\end{align*}
\end{proof}
\end{enumerate}


\newpage
\question{Task 2}
\begin{proof}
First observe that
\begin{align*}
    ||A - \widehat{A}||_2^2 &\le ||A - \widehat{A}||_F^2\\
    &\le \sum_{i,j} (A_{i,j} - \widehat{A}_{i,j})^2\\
    &\le \sum_{i,j} (\epsilon/2n)^2\\
    &= \epsilon^2/4\\
    \Rightarrow ||A - \widehat{A}||_2 &\le \epsilon/2.
\end{align*}
Now, let $Y_t = \frac{\widehat{A}_{i_t, j_t}}{p_{i_t, j_t}}e_{i_t}e_{j_t}^T, X_t = \widehat{A} - Y_t$ so $\widehat{A} - \widetilde{A} = \frac{1}{s}\sum_{t=1}^s X_t$. We check that
\begin{align*}
\mathbb{E}[(X_t)_{i,j}] = \widehat{A}_{i,j} - p_{i,j} \frac{\widehat{A}_{i,j}}{p_{i,j}} = 0.
\end{align*}
This also means $\mathbb{E}[Y_t] = \widehat{A}$. And with probability 1,
\begin{align*}
||X_t||_2^2 &\le ||X_t||_F^2\\
&\le ||\widehat{A}||_F^2 + \max_{i,j} \left(\widehat{A}_{i,j} - \frac{\widehat{A}_{i,j}}{p_{i,j}}\right)^2 - \widehat{A}_{i,j}^2\\
&\le ||\widehat{A}||_F^2 + \max_{i,j}\widehat{A}_{i,j}^2 (1/p_{i,j} - 1)\\
&\le ||\widehat{A}||_F^2 + ||\widehat{A}||_F^2 = 2||\widehat{A}||_F^2\\
\Rightarrow ||X_t||_2 &\le 2||\widehat{A}||_F.
\end{align*}
So matrix Chernoff bound conditions are satisfied. We now bound the variance as follows.
\begin{align*}
\left\| \sum_t \mathbb{E}[X_t^TX_t] \right\| &= \left\| s \mathbb{E}[X_t^TX_t] \right\|\\
&= s\|\widehat{A}^T \widehat{A} - \mathbb{E}[\widehat{A}^TY_t] - \mathbb{E}[Y_t^T \widehat{A}] + \mathbb{E}[Y^TY]\|\\
&\le s\left(\|\widehat{A}^T \widehat{A}\| + \|\mathbb{E}[\widehat{A}^TY] \| + \| \mathbb{E}[Y_t^T \widehat{A}] \| + \| \mathbb{E}[Y^TY]\|  \right)\\
&= s \left(\|\widehat{A}^T \widehat{A}\| + \|\widehat{A}^T \widehat{A}\| + \| \widehat{A}^T \widehat{A}\| + \|B\|\right), B_{i,j} = \widehat{A}_{j,i}^2/p_{j,i} = ||\widehat{A}||_F^2\\
&\le s \left(3\|\widehat{A}^T\widehat{A}\|_F + \|B\|_F\right)\\
&\le s\left(3\|\widehat{A}^T\|_F\|\widehat{A}\|_F + \sqrt{n^2 \|\widehat{A}\|_F^4}\right)\\
&= s\left(3\|\widehat{A}\|_F^2 + n\|\widehat{A}\|_F^2\right).
\end{align*}
Similarly we have $\|\sum_t \mathbb{E}[X_tX_t^T]\| \le s\left(3\|\widehat{A}\|_F^2 + n \|\widehat{A}\|_F^2\right)$, and we finally can use the matrix Chernoff bound from wikipedia to get
\begin{align*}
    \Pr[\| \widetilde{A} - \widehat{A}\|_2 \ge \epsilon/2] &= \Pr[\frac{1}{s}\|\sum X_t\|_2 \ge \epsilon/2]\\
    &= \Pr[\| \sum X_t\|_2 \ge s\epsilon/2]\\
    &\le (2n) \exp\left(- \frac{s^2\epsilon^2/8}{\sigma^2 + 2\|A\|_F s \epsilon/6}\right)\\
    &\le (2n) \exp\left(- \frac{s^2\epsilon^2/8}{s(3+n)\|\widehat{A}\|_F^2 + \|\widehat{A}\|_F s\epsilon/3}\right)\\
    &\le (2n) \exp\left( - \frac{\Omega(\|\widehat{A}\|_F^2 n \epsilon^{-2} \ln n)\epsilon^2/8}{(3 + n)\|\widehat{A}\|_F^2 + \|\widehat{A}\|_F \epsilon/3}\right)\\
    &= (2n) \exp\left( - \frac{\Omega(\|\widehat{A}\|_F n\ln n)}{(3 + n)\|\widehat{A}\|_F + \epsilon/3}\right)\\
    &\le (2n) \exp\left( - \frac{\Omega(\|\widehat{A}\|_F n\ln n)}{(3 + n)\|\widehat{A}\|_F + 2n\|\widehat{A}\|_F/3}\right)\\
    &\le (2n) \exp\left( - \Omega(\ln n)\right)\\
    &= (2n)\exp\left(- \Omega(\ln n^2)\right)\\
    &= (2n)O(1/n^2)\\
    &= O(1/n).
\end{align*}
It is clear that we can set the constants correctly such that the probability is at most $1/n$. Hence we high probability (at least $1 - 1/n$) we get
\begin{align*}
    \|A - \widetilde{A}\|_2 \le \|A - \widehat{A}\|_2 + \|\widehat{A} - \widetilde{A}\|_2 \le \epsilon/2 + \epsilon/2 = \epsilon.
\end{align*}
We're done.
\end{proof}


\newpage
\question{Task 3}
\begin{proof}
As hint says, we should first prove for vectors.

\underline{Claim:} $\forall i,j, |(AB)_{i,j} - (\overline{A}\overline{B})_{i,j}| \le \epsilon \|A_{i*}\|_1 \|B_{*j}\|_1$.
\begin{proof}
Note that for any $k$,
\begin{align*}
    |\overline{A}_{ik} \overline{B}_{kj} - A_{ik}B_{kj}| = \begin{cases}
        0, & |A_{ik}| > (\epsilon/2) \|A_{i*}\|_1, |B_{kj}| > (\epsilon/2) \|B_{*j}\|_1\\
        |A_{ik}B_{kj}|, & \text{o.w.}
    \end{cases}.
\end{align*}
Then we have
\begin{align*}
    |(AB)_{i,j} - (\overline{A}\overline{B})_{i,j}| &\le \sum_k |\overline{A}_{ik} \overline{B}_{kj} - A_{ik}B_{kj}|\\
    &\le \sum_k (\epsilon/2)\max\left\{|A_{ik}|\sum_t |B_{tj}|, \sum_t |A_{it}||B_{kj}|\right\}\\
    &\le (\epsilon/2)\sum_k \sum_t 2|A_{ik}||B_{tj}|\\
    &= \epsilon \|A_{i*}\|_1 \|B_{*j}\|_1.
\end{align*}
Note that the last inequality follows from the observation that each $|A_{ik}||B_{tj}||$ (for each $k,t$) is summed at most twice, once when the outer index is at $k$ and once when the outer index is at $t$.
\end{proof}
Now, it is clear that
\begin{align*}
\|AB - \overline{A} \cdot \overline{B}\|_1 &= \sum_{i,j} |(AB)_{ij} - (\overline{A} \cdot \overline{B})_{ij}| \\
&\le \epsilon\sum_{i,j} \|A_{i*}\|_1 \|B_{*j}\|_1\\
&= \epsilon \sum_i \|A_{i*}\|_1 \sum_j \|B_{*j}\|_1\\
&= \epsilon \|A\|_1 \|B\|_1. \qedhere
\end{align*}
\end{proof}


\newpage
\question{Task 4}
\begin{proof}
Our algorithm is as follows.
\begin{align*}
&\text{Use }36\log m\text{ independent count sketch matrices, each containing }s = 3600d^2 \text{ rows}.\\
&\text{For each such matrix }S_k:\\
&\hspace{15px}\text{Let }C = S_kA, D = S_kB.\\
&\hspace{15px}\text{Let }L \in \mathbb{R}^{O(\log m) \times s}\text{ drawn from i.i.d. Gaussians satisfying distributional JL lemma.}\\
&\hspace{15px}\text{Let }M = LCC^-D - LD \text{ via matrix multiplications left-to-right.}\\
&\hspace{15px}\text{Let }v_k \in \mathbb{R}^m \text{ with }(v_k)_i = \|M_{*i}\|_2.\\
&\text{Let }v \in \mathbb{R}^m \text{ whose entries are medians of }\{(v_k)_i\}_{k \in [s]}.\\
&\text{Return }\min_i v_i
\end{align*}

\underline{Claim:} For all columns $j$, with $5/6$ probability, $\|M_{*j}\|_2$ approximates optimal $\|Cx - D_{*j}\|_2$ within $1 \pm 0.1$.
\begin{proof}
Let $\epsilon = 0.1$. Let $\delta = \frac{1}{6m}$. Since $L$ matrix has $O(\epsilon^{-2}\log(1/\delta)) = O(\log m)$ rows, it satisfies JL property (e.g., from wikipedia). Let $w_j = (CC^-D)_{*j} - D_{*j}$, the solution to $\min_x \|Cx - D_{*j}\|_2$. By JL property, we have
\begin{align*}
    \forall j \in [m], \Pr[\|Lw_j\|_2 - \|w_j\|_2 > \|w_j\|_2\epsilon] &< \delta\\
    \Rightarrow \Pr\left[\exists j \in [m], \|Lw_j\|_2 - \|w_j\|_2 > \|w_j\|_2\epsilon\right] &< m\delta\\
    \Rightarrow \Pr\left[\forall j \in [m], \|Lw_j\|_2 - \|w_j\|_2 > \|w_j\|_2\epsilon\right] &> 1 - m\delta\\
    \Rightarrow \Pr\left[\forall j \in [m], \|M_{*j}\|_2 \in (1 \pm \epsilon) \|w_j\|_2\right] &> 5/6. \qedhere
\end{align*}
\end{proof}

\underline{Claim:} For a fixed column $j$, with 2/3 probability, $\|M_{*j}\|_2$ approximates optimal $\|Ax - B_{*j}\|_2$ within a factor of 2.
\begin{proof}
Let $\epsilon = 0.1$, $\delta = 1/6$. Since $s = 3600d^2 = 6d^2/(\delta \epsilon^2)$, our count sketch matrix satisfies subspace embedding. Let $r_j^* = \min_x\|Ax - B_{*j}\|_2$. Then
\begin{align*}
    \Pr[\|M_{*j}\|_2 \notin (1/2, 2)r_j^*] &\le \Pr[\|M_{*j}\|_2 \notin (1 \pm \epsilon)\|w_j\|_2] + \Pr[\|w_j\|_2 \notin (1 \pm \epsilon)r_j^*]\\
    &\le 1/6 + 1/6\\
    &= 1/3. \qedhere
\end{align*}
\end{proof}

\underline{Claim:} We approximate least cost regression within a factor of 2, with probability at least $9/10$.
\begin{proof}
In order for the median of $t = 36\log m$ values to fail (for some fixed $j$), we need at least half of them to fail (where failing means landing outside the factor of 2 range). Each has failure probability at most $\frac{1}{3}$, so if we let $X_{i \in [t]} = 1$ iff failure, by Hoeffding we then have
\begin{align*}
\Pr[\sum X_i \ge t/2] &= \Pr[\sum X_i - \frac{1}{3}t \ge \frac{1}{6}t]\\
&\le \Pr[\sum X_i - \mathbb{E}[\sum X_i] \ge \frac{1}{6}t]\\
&\le \exp\left(- \frac{t^2/18}{t}\right)\\
&= e^{-t/18}\\
&= m^{-2}
\end{align*}
By union bound we know that probability that at least one failure occurs out of $m$ columns is at most $m^{-1}$, and so all successful approximation has probability at least $1 - 1/m$. We may assume $m \ge 10$ here since otherwise we can just solve all regression problems and take minimum naively. It is also easy to see that if for all columns, our approximation is at least within a factor of 2, the minimum is also within a factor of 2. We're done.
\end{proof}

Finally, we show our algorithm runs quickly. Note that for each countsketch matrix, computing $C,D$ uses $nnz(A) + nnz(B)$ time and drawing Gaussians uses $O(d^2\log m)$ time. Since $nnz(C) \le nnz(A), nnz(D) \le nnz(B)$ we have $LC$ taking at most $O(nnz(A)\log m)$ time, $C^{-}$ taking $O(d^2d^2) = O(d^4)$ time, $(LC)C^-$ taking $O(dd^2\log m) = O(d^3\log m)$ time, $(LCC^-)D$ and computing $LD$ taking $O(\log(m)nnz(B))$ time, and computing $M$ taking $O(m\log m)$ time. Figuring out column norms of $M$ takes $O(m\log m)$ time. So for each $S_k$ our work is
\begin{align*}
    &O(nnz(A) + nnz(B) + d^2\log m + nnz(A)\log m + d^4 + d^3\log m + nnz(B)\log m + 2m\log m)\\
    =& O(\log m(nnz(A) + nnz(B) + d^4))
\end{align*}
assuming that $m \le nnz(B)$ since otherwise we can just cut the zero columns of $B$ as a preprocessing step. Taking elem-wise median of $\log m$ vectors of length $m$ takes time $m \polylog(m)$ so in total our algo runs in time
\begin{align*}
    &O(\log m)O(\log m(nnz(A) + nnz(B) + d^4)) + O(m \polylog(m))\\
    =&O(\polylog(m)(nnz(A) + nnz(B) + d^4))
\end{align*}
as desired. [Oh man that was crazy.]
\end{proof}


\end{document}  